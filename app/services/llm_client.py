from __future__ import annotations

import json
import logging
import re
from typing import Optional, Tuple

import httpx

from ..config import settings


logger = logging.getLogger(__name__)

_JSON_PATTERN = re.compile(r"\{.*\}", re.DOTALL)
MAX_LOG_LEN = 1000

SYSTEM_PROMPT = (
    "–¢—ã ‚Äî –≤–µ–¥—É—â–∏–π –∞–Ω–∞–ª–∏—Ç–∏–∫ –ø–æ –ø—Ä–æ–≤–µ—Ä–∫–µ —Ñ–∞–∫—Ç–æ–≤. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º JSON –≤–∏–¥–∞ "
    '{"probability": <float 0..1>, "explanation": "–ø–æ–¥—Ä–æ–±–Ω–æ–µ —Ä—É—Å—Å–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ"}. '
    "probability ‚Äî –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∏—Å—Ç–∏–Ω–Ω–æ—Å—Ç–∏ —É—Ç–≤–µ—Ä–∂ÔøΩÔøΩ–µ–Ω–∏—è (0..1). "
    "explanation ‚Äî 3-4 –Ω–∞—Å—ã—â–µ–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –≥–¥–µ —Ç—ã –∫—Ä–∞—Ç–∫–æ –æ–ø–∏—Å—ã–≤–∞–µ—à—å –∫–æ–Ω—Ç–µ–∫—Å—Ç, –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã, —É–ø–æ–º–∏–Ω–∞–µ—à—å –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏."
)


class LLMClient:
    def __init__(self):
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ OPENAI_API_KEY –∏–∑ config
        self.api_key = settings.OPENAI_API_KEY
        # –ë–∞–∑–æ–≤—ã–π URL –∏ –º–æ–¥–µ–ª—å –∂—ë—Å—Ç–∫–æ –∑–∞–¥–∞–Ω—ã
        self.api_base = "https://api.openai.com/v1"
        self.model = "gpt-3.5-turbo"
        self.timeout = 30.0

        print("ü§ñ LLMClient init:")
        print(f"   API Base: {self.api_base}")
        print(f"   Model: {self.model}")
        print(f"   API Key present: {bool(self.api_key)}")

    async def analyze(self, prompt: str) -> Optional[Tuple[float, str]]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ LLM API.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å, –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ) –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ.
        """
        if not self.api_key:
            logger.error("‚ùå OPENAI_API_KEY not set")
            return None

        logger.info("ü§ñ OPENAI API –ó–ê–ü–†–û–°:")
        logger.info(f"   üîó URL: {self.api_base}/chat/completions")
        logger.info(f"   üìã –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:")
        logger.info(f"      - model: {self.model}")
        logger.info(f"      - temperature: 0.3")
        logger.info(f"      - max_tokens: 500")
        logger.info(f"   üí¨ –ü—Ä–æ–º–ø—Ç (–ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤):")
        logger.info(f"      {prompt[:200]}{'...' if len(prompt) > 200 else ''}")

        try:
            user_prompt = (
                "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∏–∂–µ. "
                "–û–ø—Ä–µ–¥–µ–ª–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∏—Å—Ç–∏–Ω–Ω–æ—Å—Ç–∏ (0..1) –∏ —Å—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, "
                "–∫–æ—Ç–æ—Ä–æ–µ –æ—Ç—Ä–∞–∂–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã, –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–ª–∏ –∏—Ö –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ.\n\n"
                f"{prompt}"
            )

            request_payload = {
                "model": self.model,
                "messages": [
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": user_prompt},
                ],
                "temperature": 0.2,
                "max_tokens": 600,
            }

            async with httpx.AsyncClient(timeout=self.timeout) as client:
                resp = await client.post(
                    f"{self.api_base}/chat/completions",
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json",
                    },
                    json=request_payload,
                )

                logger.info(f"   üì° –û–¢–í–ï–¢ OpenAI API:")
                logger.info(f"      - –°—Ç–∞—Ç—É—Å: {resp.status_code}")
                logger.info(f"      - –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞: {resp.elapsed.total_seconds():.2f}s")

                if resp.status_code != 200:
                    logger.error(f"      ‚ùå –û—à–∏–±–∫–∞: {resp.text[:300]}")
                    return None

                data = resp.json()

                # –õ–æ–≥–∏—Ä—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤
                usage = data.get("usage", {})
                if usage:
                    logger.info(f"      üìä –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤:")
                    logger.info(f"         - prompt: {usage.get('prompt_tokens', 0)}")
                    logger.info(f"         - completion: {usage.get('completion_tokens', 0)}")
                    logger.info(f"         - total: {usage.get('total_tokens', 0)}")

                content = data.get("choices", [{}])[0].get("message", {}).get("content", "")
                finish_reason = data.get("choices", [{}])[0].get("finish_reason", "unknown")

                logger.info(f"      ‚úÖ –û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω (finish_reason: {finish_reason})")
                logger.info(f"      üí° –û—Ç–≤–µ—Ç (–ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤):")
                logger.info(f"         {content[:200]}{'...' if len(content) > 200 else ''}")
                logger.info(f"      üßæ –ü–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç (–¥–æ {MAX_LOG_LEN} —Å–∏–º–≤–æ–ª–æ–≤): {content[:MAX_LOG_LEN]}")

                parsed = self._parse_llm_response(content)
                if parsed is None:
                    logger.warning(
                        "      ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–æ–±—Ä–∞—Ç—å JSON LLM, –∏—Å–ø–æ–ª—å–∑—É–µ–º —ç–≤—Ä–∏—Å—Ç–∏–∫—É. –°—ã—Ä–æ–π –æ—Ç–≤–µ—Ç: %s",
                        content[:MAX_LOG_LEN],
                    )
                    return 0.3, "AI-–∞–Ω–∞–ª–∏–∑ –≤–µ—Ä–Ω—É–ª –æ—Ç–≤–µ—Ç, –∫–æ—Ç–æ—Ä—ã–π –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–æ–±—Ä–∞—Ç—å."

                prob, explanation = parsed
                logger.info(f"      üéØ –ò–∑–≤–ª–µ—á–µ–Ω–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {prob}")
                return prob, explanation
        except Exception as e:
            logger.error(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ OpenAI API: {e}")
            return 0.3, f"AI-–∞–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}"

    def _parse_llm_response(self, content: str) -> Optional[Tuple[float, str]]:
        try:
            json_candidate = json.loads(content)
        except json.JSONDecodeError as exc:
            logger.debug("LLM JSON decode error (primary): %s | raw=%s", exc, content[:MAX_LOG_LEN])
            match = _JSON_PATTERN.search(content)
            if not match:
                logger.debug("LLM JSON regex search –Ω–µ –Ω–∞—à—ë–ª –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ –±–ª–æ–∫–∞")
                return None
            try:
                json_candidate = json.loads(match.group())
            except json.JSONDecodeError as exc:
                logger.debug("LLM JSON decode error (regex match): %s | raw=%s", exc, match.group()[:MAX_LOG_LEN])
                return None

        probability = json_candidate.get("probability")
        explanation = json_candidate.get("explanation")

        try:
            probability = float(probability)
        except (TypeError, ValueError):
            logger.debug("LLM probability –Ω–µ float: %s (–ø–æ–¥—Å—Ç–∞–≤–ª—è–µ–º 0.5)", probability)
            probability = 0.5

        probability = max(0.0, min(1.0, probability))
        if not explanation:
            logger.debug("LLM explanation –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥–ª—É—à–∫—É")
            explanation = "LLM –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ."

        return probability, str(explanation)
