from __future__ import annotations

import json
import logging
import re
from typing import Optional, Tuple

import httpx

from ..config import settings


logger = logging.getLogger(__name__)

_JSON_PATTERN = re.compile(r"\{.*\}", re.DOTALL)
MAX_LOG_LEN = 1000

SYSTEM_PROMPT = (
    "–¢—ã ‚Äî –≤–µ–¥—É—â–∏–π –∞–Ω–∞–ª–∏—Ç–∏–∫ –ø–æ –ø—Ä–æ–≤–µ—Ä–∫–µ —Ñ–∞–∫—Ç–æ–≤. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º JSON –≤–∏–¥–∞ "
    '{"probability": <float 0..1>, "explanation": "–ø–æ–¥—Ä–æ–±–Ω–æ–µ —Ä—É—Å—Å–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ"}. '
    "probability ‚Äî –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∏—Å—Ç–∏–Ω–Ω–æ—Å—Ç–∏ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è (0..1). "
    "explanation ‚Äî 3-4 –Ω–∞—Å—ã—â–µ–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –≥–¥–µ —Ç—ã –∫—Ä–∞—Ç–∫–æ –æ–ø–∏—Å—ã–≤–∞–µ—à—å –∫–æ–Ω—Ç–µ–∫—Å—Ç, –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã, —É–ø–æ–º–∏–Ω–∞–µ—à—å –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏."
)


class LLMClient:
    def __init__(self):
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ OPENAI_API_KEY –∏–∑ config
        self.api_key = settings.OPENAI_API_KEY
        # –ë–∞–∑–æ–≤—ã–π URL –∏ –º–æ–¥–µ–ª—å –∂—ë—Å—Ç–∫–æ –∑–∞–¥–∞–Ω—ã
        self.api_base = "https://api.openai.com/v1"
        self.model = "gpt-3.5-turbo"
        self.timeout = 30.0

        print("ü§ñ LLMClient init:")
        print(f"   API Base: {self.api_base}")
        print(f"   Model: {self.model}")
        print(f"   API Key present: {bool(self.api_key)}")

    async def generalize_query(self, query: str, max_length: int = 220) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π/–æ–±–æ–±—â—ë–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è News API –ª–∏–±–æ –∏—Å—Ö–æ–¥–Ω—É—é —Å—Ç—Ä–æ–∫—É –ø—Ä–∏ —Å–±–æ—è—Ö."""
        normalized = (query or "").strip()
        if not normalized:
            return query
        if not self.api_key:
            logger.error("‚ùå OPENAI_API_KEY not set (generalize_query)")
            return query

        logger.info("ü§ñ LLM –æ–±–æ–±—â–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞: %s", normalized[:200])
        prompt = (
            "–ü—Ä–µ–æ–±—Ä–∞–∑—É–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å –∫ –Ω–æ–≤–æ—Å—Ç—è–º –≤ –Ω–∞–±–æ—Ä –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤. "
            "–°–æ—Ö—Ä–∞–Ω–∏ —Å—É—Ç—å, –¥–æ–±–∞–≤—å —Å–∏–Ω–æ–Ω–∏–º—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ–º—ã –∏ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è. "
            "–û—Ç–≤–µ—Ç—å –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π, –ø–µ—Ä–µ—á–∏—Å–ª–∏–≤ —Ç–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é (–±–µ–∑ —Ç–µ–∫—Å—Ç–∞ –¥–æ/–ø–æ—Å–ª–µ).\n"
            f"–ó–∞–ø—Ä–æ—Å: {normalized}"
        )

        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                resp = await client.post(
                    f"{self.api_base}/chat/completions",
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json",
                    },
                    json={
                        "model": self.model,
                        "messages": [
                            {
                                "role": "system",
                                "content": "–¢—ã –ø–æ–º–æ—â–Ω–∏–∫ —Ä–µ–¥–∞–∫—Ç–æ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π. –ü—Ä–µ–¥–ª–∞–≥–∞–π —Ç–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞, –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é –±–µ–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤.",
                            },
                            {"role": "user", "content": prompt},
                        ],
                        "temperature": 0.4,
                        "max_tokens": 120,
                    },
                )

                logger.info(
                    "   üì° –û—Ç–≤–µ—Ç LLM (generalize_query): —Å—Ç–∞—Ç—É—Å %s, %.2fs",
                    resp.status_code,
                    resp.elapsed.total_seconds(),
                )

                if resp.status_code != 200:
                    logger.error("   ‚ùå –û—à–∏–±–∫–∞ LLM generalize: %s", resp.text[:300])
                    return query

                data = resp.json()
                content = data.get("choices", [{}])[0].get("message", {}).get("content", "")
                generalized = content.strip().replace("\n", " ")[:max_length]
                if not generalized:
                    logger.warning("   ‚ö†Ô∏è LLM –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π –æ–±–æ–±—â—ë–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π")
                    return query

                raw_keywords = generalized.replace(";", ",").split(",")
                keywords = [kw.strip(" \"'\t") for kw in raw_keywords if kw.strip(" \"'\t")]
                if not keywords:
                    logger.warning("   ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–¥–µ–ª–∏—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å")
                    return query

                deduped_keywords = list(dict.fromkeys(keywords))
                keyword_query = " OR ".join(deduped_keywords)
                logger.info("   ‚úÖ –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: %s", deduped_keywords)
                return keyword_query
        except Exception as exc:
            logger.error("   ‚ùå generalize_query exception: %s", exc)
            return query

    async def analyze(self, prompt: str) -> Optional[Tuple[float, str]]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ LLM API.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å, –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ) –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ.
        """
        if not self.api_key:
            logger.error("‚ùå OPENAI_API_KEY not set")
            return None

        logger.info("ü§ñ OPENAI API –ó–ê–ü–†–û–°:")
        logger.info(f"   üîó URL: {self.api_base}/chat/completions")
        logger.info(f"   üìã –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:")
        logger.info(f"      - model: {self.model}")
        logger.info(f"      - temperature: 0.3")
        logger.info(f"      - max_tokens: 500")
        logger.info(f"   üí¨ –ü—Ä–æ–º–ø—Ç (–ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤):")
        logger.info(f"      {prompt[:200]}{'...' if len(prompt) > 200 else ''}")

        try:
            user_prompt = (
                "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∏–∂–µ. "
                "–û–ø—Ä–µ–¥–µ–ª–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∏—Å—Ç–∏–Ω–Ω–æ—Å—Ç–∏ (0..1) –∏ —Å—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, "
                "–∫–æ—Ç–æ—Ä–æ–µ –æ—Ç—Ä–∞–∂–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã, –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–ª–∏ –∏—Ö –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ.\n\n"
                f"{prompt}"
            )

            request_payload = {
                "model": self.model,
                "messages": [
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": user_prompt},
                ],
                "temperature": 0.2,
                "max_tokens": 600,
            }

            async with httpx.AsyncClient(timeout=self.timeout) as client:
                resp = await client.post(
                    f"{self.api_base}/chat/completions",
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json",
                    },
                    json=request_payload,
                )

                logger.info(f"   üì° –û–¢–í–ï–¢ OpenAI API:")
                logger.info(f"      - –°—Ç–∞—Ç—É—Å: {resp.status_code}")
                logger.info(f"      - –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞: {resp.elapsed.total_seconds():.2f}s")

                if resp.status_code != 200:
                    logger.error(f"      ‚ùå –û—à–∏–±–∫–∞: {resp.text[:300]}")
                    return None

                data = resp.json()

                # –õ–æ–≥–∏—Ä—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤
                usage = data.get("usage", {})
                if usage:
                    logger.info(f"      üìä –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤:")
                    logger.info(f"         - prompt: {usage.get('prompt_tokens', 0)}")
                    logger.info(f"         - completion: {usage.get('completion_tokens', 0)}")
                    logger.info(f"         - total: {usage.get('total_tokens', 0)}")

                content = data.get("choices", [{}])[0].get("message", {}).get("content", "")
                finish_reason = data.get("choices", [{}])[0].get("finish_reason", "unknown")

                logger.info(f"      ‚úÖ –û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω (finish_reason: {finish_reason})")
                logger.info(f"      üí° –û—Ç–≤–µ—Ç (–ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤):")
                logger.info(f"         {content[:200]}{'...' if len(content) > 200 else ''}")
                logger.info(f"      üßæ –ü–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç (–¥–æ {MAX_LOG_LEN} —Å–∏–º–≤–æ–ª–æ–≤): {content[:MAX_LOG_LEN]}")

                parsed = self._parse_llm_response(content)
                if parsed is None:
                    logger.warning(
                        "      ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–æ–±—Ä–∞—Ç—å JSON LLM, –∏—Å–ø–æ–ª—å–∑—É–µ–º —ç–≤—Ä–∏—Å—Ç–∏–∫—É. –°—ã—Ä–æ–π –æ—Ç–≤–µ—Ç: %s",
                        content[:MAX_LOG_LEN],
                    )
                    return 0.3, "AI-–∞–Ω–∞–ª–∏–∑ –≤–µ—Ä–Ω—É–ª –æ—Ç–≤–µ—Ç, –∫–æ—Ç–æ—Ä—ã–π –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–æ–±—Ä–∞—Ç—å."

                prob, explanation = parsed
                logger.info(f"      üéØ –ò–∑–≤–ª–µ—á–µ–Ω–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {prob}")
                return prob, explanation
        except Exception as e:
            logger.error(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ OpenAI API: {e}")
            return 0.3, f"AI-–∞–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}"

    def _parse_llm_response(self, content: str) -> Optional[Tuple[float, str]]:
        try:
            json_candidate = json.loads(content)
        except json.JSONDecodeError as exc:
            logger.debug("LLM JSON decode error (primary): %s | raw=%s", exc, content[:MAX_LOG_LEN])
            match = _JSON_PATTERN.search(content)
            if not match:
                logger.debug("LLM JSON regex search –Ω–µ –Ω–∞—à—ë–ª –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ –±–ª–æ–∫–∞")
                return None
            try:
                json_candidate = json.loads(match.group())
            except json.JSONDecodeError as exc:
                logger.debug("LLM JSON decode error (regex match): %s | raw=%s", exc, match.group()[:MAX_LOG_LEN])
                return None

        probability = json_candidate.get("probability")
        explanation = json_candidate.get("explanation")

        try:
            probability = float(probability)
        except (TypeError, ValueError):
            logger.debug("LLM probability –Ω–µ float: %s (–ø–æ–¥—Å—Ç–∞–≤–ª—è–µ–º 0.5)", probability)
            probability = 0.5

        probability = max(0.0, min(1.0, probability))
        if not explanation:
            logger.debug("LLM explanation –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥–ª—É—à–∫—É")
            explanation = "LLM –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ."

        return probability, str(explanation)
