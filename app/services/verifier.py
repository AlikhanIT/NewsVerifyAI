from __future__ import annotations

import hashlib
import json
import asyncio
import logging

from sqlalchemy.orm import Session

from ..schemas import CheckRequest, CheckResponse, ClaimAnalysis
from ..models import CachedResult
from .nlp import extract_entities, extract_ngrams
from .news_client import NewsClient
from .llm_client import LLMClient


logger = logging.getLogger(__name__)


def _hash_claim(text: str, style: str) -> str:
    h = hashlib.sha256()
    h.update(text.strip().encode("utf-8"))
    h.update(style.encode("utf-8"))
    return h.hexdigest()


def _normalize_cached_result(data: dict) -> dict:
    analysis = data.get("analysis")
    if not isinstance(analysis, dict):
        return data

    matched_sources = analysis.get("matched_sources")
    if not isinstance(matched_sources, list):
        return data

    normalized = []
    changed = False
    for item in matched_sources:
        if isinstance(item, dict):
            normalized.append(item)
        else:
            changed = True
            text = str(item)
            normalized.append(
                {
                    "title": text,
                    "description": None,
                    "summary": text,
                    "url": None,
                    "published_at": None,
                    "source_name": None,
                }
            )

    if changed:
        analysis["matched_sources"] = normalized

    return data


def _format_formal_explanation(
    claim: str,
    status: str,
    probability: float | None,
    entities: dict,
    news_count: int,
    base_explanation: str,
) -> str:
    prob_text = f"{probability:.2f}" if probability is not None else "–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞"
    ent_parts = []
    for k, vals in entities.items():
        if vals:
            ent_parts.append(f"{k}: {', '.join(vals)}")
    entities_text = "; ".join(ent_parts) if ent_parts else "–∫–ª—é—á–µ–≤—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ –Ω–µ –≤—ã–¥–µ–ª–µ–Ω—ã"

    status_human = {
        "confirmed": "–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–Ω—ã–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏",
        "not_found": "–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π –≤ –Ω–æ–≤–æ—Å—Ç—è—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–æ (–Ω–∏–∑–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Å–æ–±—ã—Ç–∏—è)",
        "uncertain": "–ø—Ä—è–º—ã—Ö –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π –Ω–µ—Ç (—Å–æ–±—ã—Ç–∏–µ —É–º–µ—Ä–µ–Ω–Ω–æ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω–æ)",
    }.get(status, status)

    return (
        f"–ü—Ä–æ–≤–µ–¥–µ–Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—ÄÔøΩÔøΩ–∞ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è:\n\n"
        f"¬´{claim}¬ª.\n\n"
        f"–ù–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –ø—É–±–ª–∏–∫–∞—Ü–∏–π –∏ –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π "
        f"—Å–∏—Å—Ç–µ–º–∞ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–ª–∞ —Å–ª–µ–¥—É—é—â–∏–π –≤—ã–≤–æ–¥: {status_human}. "
        f"–û—Ü–µ–Ω–æ—á–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∏—Å—Ç–∏–Ω–Ω–æ—Å—Ç–∏ —Å–æ–±—ã—Ç–∏—è: {prob_text}.\n\n"
        f"–í—ã–¥–µ–ª–µ–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏: {entities_text}. "
        f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –ø—É–±–ª–∏–∫–∞—Ü–∏–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω—é—é –Ω–µ–¥–µ–ª—é: {news_count}. "
        f"–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (AI):\n\n{base_explanation}\n\n"
        f"–í—ã–≤–æ–¥ –Ω–æ—Å–∏—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä –∏ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º—ã–º."
    )


def _format_simple_explanation(status: str, probability: float | None) -> str:
    if probability is None:
        return "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏."

    if status == "confirmed":
        base = "–ù–∞—à–ª–∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –≤ –Ω–æ–≤–æ—Å—Ç—è—Ö."
    elif status == "not_found":
        base = "–í –Ω–æ–≤–æ—Å—Ç—è—Ö –Ω–µ –Ω–∞—à–ª–∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π. –°–æ–±—ã—Ç–∏–µ –º–∞–ª–æ–≤–µ—Ä–æ—è—Ç–Ω–æ."
    else:
        base = "–ü—Ä—è–º—ã—Ö –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π –Ω–µ—Ç."

    if probability >= 0.8:
        prob_text = "–í–µ—Ä–æ—è—Ç–Ω–æ, —ç—Ç–æ –ø—Ä–∞–≤–¥–∞."
    elif probability >= 0.5:
        prob_text = "–ü–æ—Ö–æ–∂–µ –Ω–∞ –ø—Ä–∞–≤–¥—É, –Ω–æ –Ω–µ —Ç–æ—á–Ω–æ."
    elif probability >= 0.3:
        prob_text = "–°–æ–º–Ω–∏—Ç–µ–ª—å–Ω–æ."
    else:
        prob_text = "–ú–∞–ª–æ–≤–µ—Ä–æ—è—Ç–Ω–æ."

    return f"{base} {prob_text}".strip()


async def verify_claim(
    db: Session,
    payload: CheckRequest,
    news_client: NewsClient | None = None,
    llm_client: LLMClient | None = None,
) -> CheckResponse:
    news_client = news_client or NewsClient()
    llm_client = llm_client or LLMClient()

    claim = payload.text.strip()
    style = payload.style

    logger.info("üîç –ù–ê–ß–ê–õ–û –ü–†–û–í–ï–†–ö–ò –£–¢–í–ï–†–ñ–î–ï–ù–ò–Ø")
    logger.info(f"   üìù –£—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ: {claim[:100]}{'...' if len(claim) > 100 else ''}")
    logger.info(f"   üé® –°—Ç–∏–ª—å: {style}")

    claim_hash = _hash_claim(claim, style)
    logger.info(f"   üîê Hash: {claim_hash}")

    cached = db.query(CachedResult).filter_by(claim_hash=claim_hash).first()
    if cached:
        logger.info("   ‚ö° –ù–ê–ô–î–ï–ù –í –ö–≠–®–ï - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
        data = json.loads(cached.result_json)
        data['cached'] = True
        data = _normalize_cached_result(data)
        return CheckResponse(**data)

    logger.info("   üÜï –ù–ï–¢ –í –ö–≠–®–ï - –≤—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–ª–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É")

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏ n-–≥—Ä–∞–º–º
    logger.info("   üîé –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—É—â–Ω–æ—Å—Ç–∏ –∏ n-–≥—Ä–∞–º–º—ã...")
    entities = extract_entities(claim)
    ngrams = extract_ngrams(claim, n=2)
    logger.info(f"      üìå –°—É—â–Ω–æ—Å—Ç–∏: {entities}")
    logger.info(f"      üîó N-–≥—Ä–∞–º–º—ã: {ngrams[:5]}{'...' if len(ngrams) > 5 else ''}")

    # –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π
    logger.info("   üì∞ –ò—â–µ–º –Ω–æ–≤–æ—Å—Ç–∏...")
    news_results = await news_client.search(claim, from_days=7, limit=5)

    if news_results:
        logger.info(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(news_results)} –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π")
        for idx, nr in enumerate(news_results, 1):
            logger.info(
                "      %s. [%s] %s",
                idx,
                (nr.source_name or "Unknown"),
                (nr.title or "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è")[:100],
            )
        status = "confirmed"
        probability = 0.9
        explanation_base = (
            f"–ü–æ –Ω–∞–π–¥–µ–Ω–Ω—ã–º –Ω–æ–≤–æ—Å—Ç–Ω—ã–º –ø—É–±–ª–∏–∫–∞—Ü–∏—è–º —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç—Å—è. "
            f"–ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {len(news_results)}."
        )
    else:
        logger.info("   ‚ùå –ù–æ–≤–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ - –æ–±—Ä–∞—â–∞–µ–º—Å—è –∫ LLM –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        context_summary = "No matching news articles found for this claim within the last 7 days."

        try:
            timeout_seconds = 10
            logger.info(f"   ‚è±Ô∏è –ó–∞–ø—É—Å–∫–∞–µ–º LLM –∞–Ω–∞–ª–∏–∑ (—Ç–∞–π–º–∞—É—Ç: {timeout_seconds}s)...")
            result = await asyncio.wait_for(
                llm_client.analyze(f"Claim: {claim}\nContext: {context_summary}"),
                timeout=timeout_seconds,
            )

            if (
                result is None
                or not isinstance(result, (list, tuple))
                or len(result) != 2
                or result[0] is None
            ):
                raise ValueError("LLM returned unexpected result")

            probability, llm_explanation = result

            if not llm_explanation:
                llm_explanation = "AI-–∞–Ω–∞–ª–∏–∑ –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ."

            logger.info(f"   üéØ LLM –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {probability}")
            logger.info(f"   üí¨ LLM –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ (–ø–µ—Ä–≤—ã–µ 150 —Å–∏–º–≤–æ–ª–æ–≤): {llm_explanation[:150]}...")

        except asyncio.TimeoutError:
            logger.warning(f"   ‚è±Ô∏è LLM –¢–ê–ô–ú–ê–£–¢ (>{timeout_seconds}s)")
            probability = 0.3
            llm_explanation = "AI-–∞–Ω–∞–ª–∏–∑ –Ω–µ –∑–∞–≤–µ—Ä—à–∏–ª—Å—è –∑–∞ –æ—Ç–≤–µ–¥—ë–Ω–Ω–æ–µ –≤—Ä–µ–º—è (—Ç–∞–π–º–∞—É—Ç). –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞."
        except Exception as e:
            logger.error(f"   ‚ùå LLM –û–®–ò–ë–ö–ê: {e}")
            probability = 0.3
            llm_explanation = f"AI-–∞–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}"

        if not llm_explanation:
            llm_explanation = "AI-–∞–Ω–∞–ª–∏–∑ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –∏–ª–∏ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞."

        status = "uncertain" if (probability is not None and probability >= 0.4) else "not_found"
        explanation_base = llm_explanation

    logger.info(f"   üìä –ò–¢–û–ì–û–í–´–ô –°–¢–ê–¢–£–°: {status} (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å={probability:.2f})")

    logger.info(f"   üìù –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ (—Å—Ç–∏–ª—å: {style})...")
    explanation = (
        _format_formal_explanation(claim, status, probability, entities, len(news_results), explanation_base)
        if style == "formal"
        else _format_simple_explanation(status, probability)
    )

    analysis = ClaimAnalysis(
        status=status,
        probability=probability,
        explanation=explanation,
        matched_sources=news_results,
    )

    resp_obj = CheckResponse(
        claim=claim,
        style=style,
        analysis=analysis,
        cached=False,
    )

    logger.info("   üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –∫—ç—à –ë–î...")
    db.add(CachedResult(claim_hash=claim_hash, result_json=resp_obj.json()))
    db.commit()
    logger.info("   ‚úÖ –ü–†–û–í–ï–†–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê –£–°–ü–ï–®–ù–û")

    return resp_obj
