from __future__ import annotations

import hashlib
import json
import asyncio
import logging
import re

from sqlalchemy.orm import Session

from ..schemas import CheckRequest, CheckResponse, ClaimAnalysis
from ..models import CachedResult
from .nlp import extract_entities, extract_ngrams
from .news_client import NewsClient
from .llm_client import LLMClient


logger = logging.getLogger(__name__)

STOP_WORDS = {
    "–Ω–∞", "–∏", "–≤", "–≤–æ", "–∫", "–∫–æ", "–ø–æ", "–∏–∑", "–∑–∞", "–æ—Ç", "–¥–ª—è", "–∫–∞–∫", "—á—Ç–æ",
    "—ç—Ç–æ", "—ç—Ç–æ—Ç", "—ç—Ç–∞", "—ç—Ç–∏", "–ø–æ—Å–ª–µ–¥–Ω–∏–π", "–º–µ—Å—è—Ü", "–º–µ—Å—è—Ü–∞", "–≥–æ–¥", "–≥–æ–¥–∞",
}
WORD_PATTERN = re.compile(r"[\w%]+", re.UNICODE)


def _hash_claim(text: str, style: str) -> str:
    h = hashlib.sha256()
    h.update(text.strip().encode("utf-8"))
    h.update(style.encode("utf-8"))
    return h.hexdigest()


def _is_cache_entry_modern(data: dict) -> bool:
    analysis = data.get("analysis")
    if not isinstance(analysis, dict):
        return False

    matched_sources = analysis.get("matched_sources")
    if matched_sources is None:
        return True
    if not isinstance(matched_sources, list):
        return False

    for item in matched_sources:
        if not isinstance(item, dict):
            return False
        if not item.get("title"):
            return False
        # url –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º —É —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π, –Ω–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–æ–ª–∂–Ω–∞ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å
        if "url" not in item:
            return False
    return True


def _normalize_cached_result(data: dict) -> dict:
    analysis = data.get("analysis")
    if not isinstance(analysis, dict):
        return data

    matched_sources = analysis.get("matched_sources")
    if not isinstance(matched_sources, list):
        return data

    normalized = []
    changed = False
    for item in matched_sources:
        if isinstance(item, dict):
            normalized.append(item)
        else:
            changed = True
            text = str(item)
            normalized.append(
                {
                    "title": text,
                    "description": None,
                    "summary": text,
                    "url": None,
                    "published_at": None,
                    "source_name": None,
                }
            )

    if changed:
        analysis["matched_sources"] = normalized

    return data


def _format_formal_explanation(
    claim: str,
    status: str,
    probability: float | None,
    entities: dict,
    news_count: int,
    base_explanation: str,
) -> str:
    prob_text = f"{probability:.2f}" if probability is not None else "–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞"
    ent_parts = []
    for k, vals in entities.items():
        if vals:
            ent_parts.append(f"{k}: {', '.join(vals)}")
    entities_text = "; ".join(ent_parts) if ent_parts else "–∫–ª—é—á–µ–≤—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ –Ω–µ –≤—ã–¥–µ–ª–µ–Ω—ã"

    status_human = {
        "confirmed": "–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–Ω—ã–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏",
        "not_found": "–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π –≤ –Ω–æ–≤–æ—Å—Ç—è—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–æ (–Ω–∏–∑–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Å–æ–±—ã—Ç–∏—è)",
        "uncertain": "–ø—Ä—è–º—ã—Ö –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π –Ω–µ—Ç (—Å–æ–±—ã—Ç–∏–µ —É–º–µ—Ä–µ–Ω–Ω–æ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω–æ)",
    }.get(status, status)

    return (
        f"–ü—Ä–æ–≤–µ–¥–µ–Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è:\n\n"
        f"¬´{claim}¬ª.\n\n"
        f"–ù–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –ø—É–±–ª–∏–∫–∞—Ü–∏–π –∏ –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π "
        f"—Å–∏—Å—Ç–µ–º–∞ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–ª–∞ —Å–ª–µ–¥—É—é—â–∏–π –≤—ã–≤–æ–¥: {status_human}. "
        f"–û—Ü–µ–Ω–æ—á–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∏—Å—Ç–∏–Ω–Ω–æ—Å—Ç–∏ —Å–æ–±—ã—Ç–∏—è: {prob_text}.\n\n"
        f"–í—ã–¥–µ–ª–µ–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏: {entities_text}. "
        f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –ø—É–±–ª–∏–∫–∞—Ü–∏–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω—é—é –Ω–µ–¥–µ–ª—é: {news_count}. "
        f"–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (AI):\n\n{base_explanation}\n\n"
        f"–í—ã–≤–æ–¥ –Ω–æ—Å–∏—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä –∏ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º—ã–º."
    )


def _format_simple_explanation(status: str, probability: float | None) -> str:
    if probability is None:
        return "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏."

    if status == "confirmed":
        base = "–ù–∞—à–ª–∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –≤ –Ω–æ–≤–æ—Å—Ç—è—Ö."
    elif status == "not_found":
        base = "–í –Ω–æ–≤–æ—Å—Ç—è—Ö –Ω–µ –Ω–∞—à–ª–∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π. –°–æ–±—ã—Ç–∏–µ –º–∞–ª–æ–≤–µ—Ä–æ—è—Ç–Ω–æ."
    else:
        base = "–ü—Ä—è–º—ã—Ö –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π –Ω–µ—Ç."

    if probability >= 0.8:
        prob_text = "–í–µ—Ä–æ—è—Ç–Ω–æ, —ç—Ç–æ –ø—Ä–∞–≤–¥–∞."
    elif probability >= 0.5:
        prob_text = "–ü–æ—Ö–æ–∂–µ –Ω–∞ –ø—Ä–∞–≤–¥—É, –Ω–æ –Ω–µ —Ç–æ—á–Ω–æ."
    elif probability >= 0.3:
        prob_text = "–°–æ–º–Ω–∏—Ç–µ–ª—å–Ω–æ."
    else:
        prob_text = "–ú–∞–ª–æ–≤–µ—Ä–æ—è—Ç–Ω–æ."

    return f"{base} {prob_text}".strip()


def _collect_keywords(claim: str, entities: dict[str, list[str]]) -> list[str]:
    tokens: set[str] = set()
    for token in WORD_PATTERN.findall(claim.lower()):
        if len(token) < 3 or token in STOP_WORDS:
            continue
        tokens.add(token)

    for values in entities.values():
        for value in values:
            for token in WORD_PATTERN.findall(value.lower()):
                if len(token) < 3 or token in STOP_WORDS:
                    continue
                tokens.add(token)

    return list(tokens)


def _filter_news_by_keywords(news_list, keywords):
    if not keywords:
        return news_list

    min_hits = 1 if len(keywords) <= 3 else 2
    filtered = []
    for item in news_list:
        text = " ".join(filter(None, [item.title, item.description, item.summary, item.source_name])).lower()
        hits = sum(1 for kw in keywords if kw in text)
        if hits >= min_hits:
            filtered.append(item)

    return filtered


async def verify_claim(
    db: Session,
    payload: CheckRequest,
    news_client: NewsClient | None = None,
    llm_client: LLMClient | None = None,
) -> CheckResponse:
    news_client = news_client or NewsClient()
    llm_client = llm_client or LLMClient()

    claim = payload.text.strip()
    style = payload.style

    logger.info("üîç –ù–ê–ß–ê–õ–û –ü–†–û–í–ï–†–ö–ò –£–¢–í–ï–†–ñ–î–ï–ù–ò–Ø")
    logger.info(f"   üìù –£—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ: {claim[:100]}{'...' if len(claim) > 100 else ''}")
    logger.info(f"   üé® –°—Ç–∏–ª—å: {style}")

    claim_hash = _hash_claim(claim, style)
    logger.info(f"   üîê Hash: {claim_hash}")

    cached = db.query(CachedResult).filter_by(claim_hash=claim_hash).first()
    if cached:
        logger.info("   ‚ö° –ù–ê–ô–î–ï–ù –í –ö–≠–®–ï - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
        data = json.loads(cached.result_json)
        if _is_cache_entry_modern(data):
            data['cached'] = True
            return CheckResponse(**data)

        logger.info("   ‚ôªÔ∏è Legacy –∫—ç—à –±–µ–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ ‚Äî –æ–±–Ω–æ–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
        db.delete(cached)
        db.commit()

    logger.info("   üÜï –ù–ï–¢ –í –ö–≠–®–ï - –≤—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–ª–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É")

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏ n-–≥—Ä–∞–º–º
    logger.info("   üîé –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—É—â–Ω–æ—Å—Ç–∏ –∏ n-–≥—Ä–∞–º–º—ã...")
    entities = extract_entities(claim)
    ngrams = extract_ngrams(claim, n=2)
    logger.info(f"      üìå –°—É—â–Ω–æ—Å—Ç–∏: {entities}")
    logger.info(f"      üîó N-–≥—Ä–∞–º–º—ã: {ngrams[:5]}{'...' if len(ngrams) > 5 else ''}")

    # –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π
    logger.info("   üì∞ –ò—â–µ–º –Ω–æ–≤–æ—Å—Ç–∏...")
    keyword_candidates = _collect_keywords(claim, entities)
    logger.info(f"      üîç –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {keyword_candidates}")
    news_results = await news_client.search(claim, from_days=7, limit=8)
    filtered_news = _filter_news_by_keywords(news_results, keyword_candidates)

    if filtered_news:
        logger.info(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(filtered_news)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –∏–∑ {len(news_results)}")
        for idx, nr in enumerate(filtered_news, 1):
            logger.info(
                "      %s. [%s] %s",
                idx,
                (nr.source_name or "Unknown"),
                (nr.title or "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è")[:100],
            )
        status = "confirmed"
        probability = 0.9
        explanation_base = (
            f"–ü–æ –Ω–∞–π–¥–µ–Ω–Ω—ã–º –Ω–æ–≤–æ—Å—Ç–Ω—ã–º –ø—É–±–ª–∏–∫–∞—Ü–∏—è–º —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç—Å—è. "
            f"–ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {len(filtered_news)}."
        )
        news_payload = filtered_news[:5]
    else:
        logger.info("   ‚ùå –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∏–ª–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ")
        context_summary = "No matching news articles found for this claim within the last 7 days."

        try:
            timeout_seconds = 10
            logger.info(f"   ‚è±Ô∏è –ó–∞–ø—É—Å–∫–∞–µ–º LLM –∞–Ω–∞–ª–∏–∑ (—Ç–∞–π–º–∞—É—Ç: {timeout_seconds}s)...")
            result = await asyncio.wait_for(
                llm_client.analyze(f"Claim: {claim}\nContext: {context_summary}"),
                timeout=timeout_seconds,
            )

            if not result or not isinstance(result, (list, tuple)):
                logger.warning("   ‚ö†Ô∏è LLM –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º —ç–≤—Ä–∏—Å—Ç–∏–∫—É")
                probability, llm_explanation = 0.3, "AI-–∞–Ω–∞–ª–∏–∑ –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤."
            else:
                probability, llm_explanation = result

            if not llm_explanation:
                llm_explanation = "AI-–∞–Ω–∞–ª–∏–∑ –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ."

            logger.info(f"   üéØ LLM –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {probability}")
            logger.info(f"   üí¨ LLM –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ (–ø–µ—Ä–≤—ã–µ 150 —Å–∏–º–≤–æ–ª–æ–≤): {llm_explanation[:150]}...")

        except asyncio.TimeoutError:
            logger.warning(f"   ‚è±Ô∏è LLM –¢–ê–ô–ú–ê–£–¢ (>{timeout_seconds}s)")
            probability = 0.3
            llm_explanation = "AI-–∞–Ω–∞–ª–∏–∑ –Ω–µ –∑–∞–≤–µ—Ä—à–∏–ª—Å—è –∑–∞ –æ—Ç–≤–µ–¥—ë–Ω–Ω–æ–µ –≤—Ä–µ–º—è (—Ç–∞–π–º–∞—É—Ç). –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞."
        except Exception as e:
            logger.error(f"   ‚ùå LLM –û–®–ò–ë–ö–ê: {e}")
            probability = 0.3
            llm_explanation = f"AI-–∞–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}"

        if not llm_explanation:
            llm_explanation = "AI-–∞–Ω–∞–ª–∏–∑ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –∏–ª–∏ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞."

        status = "uncertain" if (probability is not None and probability >= 0.4) else "not_found"
        explanation_base = llm_explanation
        news_payload = []

    logger.info(f"   üìä –ò–¢–û–ì–û–í–´–ô –°–¢–ê–¢–£–°: {status} (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å={probability:.2f})")

    logger.info(f"   üìù –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ (—Å—Ç–∏–ª—å: {style})...")
    explanation = (
        _format_formal_explanation(claim, status, probability, entities, len(news_payload), explanation_base)
        if style == "formal"
        else _format_simple_explanation(status, probability)
    )

    analysis = ClaimAnalysis(
        status=status,
        probability=probability,
        explanation=explanation,
        matched_sources=news_payload,
    )

    resp_obj = CheckResponse(
        claim=claim,
        style=style,
        analysis=analysis,
        cached=False,
    )

    logger.info("   üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –∫—ç—à –ë–î...")
    db.add(CachedResult(claim_hash=claim_hash, result_json=resp_obj.json()))
    db.commit()
    logger.info("   ‚úÖ –ü–†–û–í–ï–†–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê –£–°–ü–ï–®–ù–û")

    return resp_obj
